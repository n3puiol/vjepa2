# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.


from logging import getLogger

import torch
import torchvision.transforms as transforms

import vjepa2.src.vjepa_datasets.utils.video.transforms as video_transforms
import vjepa2.src.vjepa_datasets.utils.video.volume_transforms as volume_transforms
from vjepa2.evals.action_anticipation_frozen.epickitchens import filter_annotations as ek100_filter_annotations
from vjepa2.evals.action_anticipation_frozen.epickitchens import make_webvid as ek100_make_webvid
from vjepa2.src.vjepa_datasets.utils.video.randerase import RandomErasing

_GLOBAL_SEED = 0
logger = getLogger()


def init_data(
    base_path,
    annotations_path,
    batch_size,
    dataset,
    frames_per_clip=16,
    fps=5,
    crop_size=224,
    rank=0,
    world_size=1,
    drop_last=True,
    num_workers=10,
    pin_mem=True,
    persistent_workers=True,
    training=True,
    decode_video=True,
    anticipation_time_sec=0.0,
    decode_one_clip=False,
    random_resize_scale=(0.9, 1.0),
    reprob=0,
    auto_augment=False,
    motion_shift=False,
    anticipation_point=[0.1, 0.1],
):
    # -- make video transformations
    transform = make_transforms(
        training=training,
        random_horizontal_flip=True,
        random_resize_aspect_ratio=(3 / 4, 4 / 3),
        random_resize_scale=random_resize_scale,
        reprob=reprob,
        auto_augment=auto_augment,
        motion_shift=motion_shift,
        crop_size=crop_size,
    )

    make_webvid = None
    if "ek100" in dataset.lower():
        make_webvid = ek100_make_webvid

    dataset, data_loader, data_info = make_webvid(
        training=training,
        decode_one_clip=decode_one_clip,
        world_size=world_size,
        rank=rank,
        base_path=base_path,
        annotations_path=annotations_path,
        batch_size=batch_size,
        transform=transform,
        frames_per_clip=frames_per_clip,
        num_workers=num_workers,
        fps=fps,
        decode_video=decode_video,
        anticipation_time_sec=anticipation_time_sec,
        persistent_workers=persistent_workers,
        pin_memory=pin_mem,
        anticipation_point=anticipation_point,
    )

    return dataset, data_loader, data_info


def filter_annotations(
    dataset,
    base_path,
    train_annotations_path,
    val_annotations_path,
    **kwargs,
):
    _filter = None
    if "ek100" in dataset.lower():
        _filter = ek100_filter_annotations

    return _filter(
        base_path=base_path,
        train_annotations_path=train_annotations_path,
        val_annotations_path=val_annotations_path,
        **kwargs,
    )


def make_transforms(
    training=True,
    random_horizontal_flip=True,
    random_resize_aspect_ratio=(3 / 4, 4 / 3),
    random_resize_scale=(0.3, 1.0),
    reprob=0.0,
    auto_augment=False,
    motion_shift=False,
    crop_size=224,
    normalize=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
):

    transform = VideoTransform(
        training=training,
        random_horizontal_flip=random_horizontal_flip,
        random_resize_aspect_ratio=random_resize_aspect_ratio,
        random_resize_scale=random_resize_scale,
        reprob=reprob,
        auto_augment=auto_augment,
        motion_shift=motion_shift,
        crop_size=crop_size,
        normalize=normalize,
    )

    return transform


class VideoTransform(object):

    def __init__(
        self,
        training=True,
        random_horizontal_flip=True,
        random_resize_aspect_ratio=(3 / 4, 4 / 3),
        random_resize_scale=(0.3, 1.0),
        reprob=0.0,
        auto_augment=False,
        motion_shift=False,
        crop_size=224,
        normalize=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ):

        self.training = training

        short_side_size = int(crop_size * 256 / 224)
        self.eval_transform = video_transforms.Compose(
            [
                video_transforms.Resize(short_side_size, interpolation="bilinear"),
                video_transforms.CenterCrop(size=(crop_size, crop_size)),
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(mean=normalize[0], std=normalize[1]),
            ]
        )

        self.random_horizontal_flip = random_horizontal_flip
        self.random_resize_aspect_ratio = random_resize_aspect_ratio
        self.random_resize_scale = random_resize_scale
        self.auto_augment = auto_augment
        self.motion_shift = motion_shift
        self.crop_size = crop_size
        self.normalize = torch.tensor(normalize)

        self.autoaug_transform = video_transforms.create_random_augment(
            input_size=(crop_size, crop_size),
            auto_augment="rand-m7-n4-mstd0.5-inc1",
            interpolation="bicubic",
        )

        self.spatial_transform = (
            video_transforms.random_resized_crop_with_shift if motion_shift else video_transforms.random_resized_crop
        )

        self.reprob = reprob
        self.erase_transform = RandomErasing(
            reprob,
            mode="pixel",
            max_count=1,
            num_splits=1,
            device="cpu",
        )

    def __call__(self, buffer):

        if not self.training:
            return self.eval_transform(buffer)

        buffer = [transforms.ToPILImage()(frame) for frame in buffer]

        if self.auto_augment:
            buffer = self.autoaug_transform(buffer)

        buffer = [transforms.ToTensor()(img) for img in buffer]
        buffer = torch.stack(buffer)  # T C H W
        buffer = buffer.permute(0, 2, 3, 1)  # T H W C

        buffer = tensor_normalize(buffer, self.normalize[0], self.normalize[1])
        buffer = buffer.permute(3, 0, 1, 2)  # T H W C -> C T H W

        buffer = self.spatial_transform(
            images=buffer,
            target_height=self.crop_size,
            target_width=self.crop_size,
            scale=self.random_resize_scale,
            ratio=self.random_resize_aspect_ratio,
        )
        if self.random_horizontal_flip:
            buffer, _ = video_transforms.horizontal_flip(0.5, buffer)

        if self.reprob > 0:
            buffer = buffer.permute(1, 0, 2, 3)
            buffer = self.erase_transform(buffer)
            buffer = buffer.permute(1, 0, 2, 3)

        return buffer


def tensor_normalize(tensor, mean, std):
    """
    Normalize a given tensor by subtracting the mean and dividing the std.
    Args:
        tensor (tensor): tensor to normalize.
        mean (tensor or list): mean value to subtract.
        std (tensor or list): std to divide.
    """
    if tensor.dtype == torch.uint8:
        tensor = tensor.float()
        tensor = tensor / 255.0
    if isinstance(mean, list):
        mean = torch.tensor(mean)
    if isinstance(std, list):
        std = torch.tensor(std)
    tensor = tensor - mean
    tensor = tensor / std
    return tensor
